{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from sklearn.metrics import mean_squared_error\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/raw'\n",
    "data = pd.read_parquet(f\"{path}/input_dataset-2.parquet\").reset_index()\n",
    "predict_inputs = pd.read_parquet(f\"{path}/prediction_input.parquet\").reset_index()\n",
    "data[\"timepoints\"] = data[\"timepoints\"].astype(np.int64)\n",
    "predict_inputs[\"timepoints\"] = predict_inputs[\"timepoints\"].astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a linear normalizer. Since our models had issues learning the long term linear trend of the dataset, we instead normalize it with a linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class LinearNormalizer():\n",
    "    def __init__(self,data):\n",
    "        self.df = data\n",
    "        nan_idx = ~np.isnan(data.iloc[:,1].values)\n",
    "        X = data.iloc[:,0].values\n",
    "        Y = data.iloc[:,9:15].values\n",
    "        X = X[nan_idx].reshape(-1,1)\n",
    "        Y = Y[nan_idx, :]\n",
    "        regr = linear_model.LinearRegression()\n",
    "        self.pipeline = Pipeline([(\"preprocess\", StandardScaler()), ('poly', PolynomialFeatures(degree=1)), (\"model\", regr)])\n",
    "        self.pipeline.fit(X, Y)\n",
    "\n",
    "    def transform(self, X, Y):\n",
    "        return Y - self.pipeline.predict(X)\n",
    "    \n",
    "    def inverse_transform(self, X, Y):\n",
    "        return Y + self.pipeline.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define main LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #pytorch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, fully_connected_size):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, fully_connected_size) #fully connected 1\n",
    "        self.fc = nn.Linear(fully_connected_size, num_classes) #fully connected last layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        hn = output.reshape(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) #first Dense\n",
    "        out = self.relu(out) #relu\n",
    "        out = self.fc(out) #Final Output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of 'mode' and split values in X and Y\n",
    "mode_one_hot = pd.get_dummies(data[\"mode\"])\n",
    "X = np.hstack([data.iloc[:,1:7].values, mode_one_hot.values])\n",
    "Y_true = data.iloc[:,9:15].values\n",
    "\n",
    "# Remove nans\n",
    "nan_idx = ~np.isnan(X).any(axis=1)\n",
    "X = X[nan_idx, :]\n",
    "Y_true = Y_true[nan_idx, :]\n",
    "N = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data with Linear Normalizer\n",
    "linear_normalizer = LinearNormalizer(data)\n",
    "X_time = data.iloc[:,0].values[nan_idx,None]\n",
    "Y_normalized = linear_normalizer.transform(X_time, Y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data to 0 mean unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x_scaler = StandardScaler().fit(X)\n",
    "y_scaler = StandardScaler().fit(Y_normalized)\n",
    "\n",
    "X = x_scaler.transform(X)\n",
    "Y = y_scaler.transform(Y_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in training and test\n",
    "train_split_fraction = 1\n",
    "\n",
    "X_train = X[:int(N*train_split_fraction)]\n",
    "Y_train = Y[:int(N*train_split_fraction)]\n",
    "\n",
    "X_val = X[int(N*train_split_fraction):]\n",
    "Y_val = Y[int(N*train_split_fraction):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable \n",
    "\n",
    "seq_length = 100\n",
    "X = X_train[:seq_length*(N//seq_length)]\n",
    "Y = Y_train[:seq_length*(N//seq_length)]\n",
    "\n",
    "# Reshape into (num_batches, seq_length, input_size)\n",
    "X_tensor = Variable(torch.from_numpy(X).reshape(-1,seq_length,X.shape[1]).float())\n",
    "Y_tensor = Variable(torch.from_numpy(Y).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10 #1000 epochs\n",
    "learning_rate = 0.001 #0.001 lr\n",
    "\n",
    "input_size = 8 #number of features\n",
    "hidden_size = 12 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "num_classes = 6 #number of output classes \n",
    "\n",
    "lstm1 = LSTM1(num_classes, input_size, hidden_size, num_layers, X_tensor.shape[1]) #our lstm class \n",
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training loop, using LBFGS as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.9891471266746521\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.LBFGS(lstm1.parameters(), lr=0.08)\n",
    "for epoch in range(num_epochs):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        out = lstm1(X_tensor)\n",
    "        loss = criterion(out, Y_tensor)\n",
    "        loss.backward()\n",
    "        print(f'Loss {loss.detach().item()}')\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure) #improve from loss, i.e backprop\n",
    "    with torch.no_grad():\n",
    "        loss = closure()\n",
    "    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) \n",
    "\n",
    "torch.save(lstm1.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting results and calculation of RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    lstm1.load_state_dict(torch.load(\"model.pth\"))\n",
    "    print(\"Model loaded\")\n",
    "\n",
    "    # Predicting test data and scaling back\n",
    "    X_tensor = Variable(torch.from_numpy(X_train).reshape(1,-1,X_train.shape[1]).float())\n",
    "    preds = lstm1(X_tensor).detach().numpy()\n",
    "    preds = y_scaler.inverse_transform(preds)\n",
    "    preds = linear_normalizer.inverse_transform(X_time[:int(N*train_split_fraction)], preds)\n",
    "\n",
    "    # Calculate RMSE for train set\n",
    "    err = mean_squared_error(Y_true[:int(N*train_split_fraction)], preds)\n",
    "    print(err, np.sqrt(err))\n",
    "\n",
    "    # Plotting predictions against training data\n",
    "    for i in range(6):\n",
    "        plt.plot(data[\"timepoints\"][nan_idx][:int(N*train_split_fraction)], y_scaler.inverse_transform(Y_true[:int(N*train_split_fraction)])[:,i])\n",
    "        plt.plot(data[\"timepoints\"][nan_idx][:int(N*train_split_fraction)], y_scaler.inverse_transform(preds)[:,i])\n",
    "        plt.show()\n",
    "\n",
    "    # Predicting train data and scaling back\n",
    "    X_val_tensor = Variable(torch.from_numpy(X_val).reshape(1,-1,X_val.shape[1]).float())\n",
    "    preds = lstm1(X_val_tensor).detach().numpy()\n",
    "    preds = y_scaler.inverse_transform(preds)\n",
    "    preds = linear_normalizer.inverse_transform(X_time[int(N*train_split_fraction):], preds)\n",
    "\n",
    "    # Calculate RMSE test set\n",
    "    err = mean_squared_error(Y_true[int(N*train_split_fraction):], preds)\n",
    "    print(err, np.sqrt(err))\n",
    "\n",
    "    # Plotting predictions against test data\n",
    "    for i in range(6):\n",
    "        plt.plot(data[\"timepoints\"][nan_idx][int(N*train_split_fraction):], y_scaler.inverse_transform(Y_true[int(N*train_split_fraction):])[:,i])\n",
    "        plt.plot(data[\"timepoints\"][nan_idx][int(N*train_split_fraction):], y_scaler.inverse_transform(preds)[:,i])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make csv for delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predict_inputs = pd.read_parquet(\"datasets/raw/prediction_input.parquet\").reset_index()\n",
    "    df = pd.DataFrame({\"timepoints\": predict_inputs[\"timepoints\"]})\n",
    "    \n",
    "    mode = pd.get_dummies(predict_inputs[\"mode\"])\n",
    "    predict_inputs[\"timepoints\"] = predict_inputs[\"timepoints\"].astype(np.int64)\n",
    "    X_test = np.hstack([predict_inputs.iloc[:,1:7].values, mode.values])\n",
    "    X_test = x_scaler.transform(X_test)\n",
    "    X_test_tensor = Variable(torch.from_numpy(X_test).reshape(1,X_test.shape[0],X_test.shape[1]).float())\n",
    "\n",
    "    X_test_time = predict_inputs.iloc[:,0].values[:,None]\n",
    "\n",
    "    preds = lstm1(X_test_tensor).detach().numpy()\n",
    "    preds = y_scaler.inverse_transform(preds)\n",
    "    preds = linear_normalizer.inverse_transform(X_test_time, preds)\n",
    "\n",
    "    for i in range(6):\n",
    "        plt.plot(preds[:,i])\n",
    "        plt.show()\n",
    "\n",
    "    for i in range(6):\n",
    "        df[f\"Bolt_{i+1}_Tensile\"] = preds[:,i]\n",
    "\n",
    "    df.to_csv(\"predict_result.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "147982e68fa9872e98b750bd3f0d345c49963796b2380f025095126cba8707af"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
